---
layout: page
title: The Kingdom of Lesser Thrones
date: 2025-11-14
categories: [Essays]
---

At some point in the coming decades, we will share the world with minds that are not human.
That sentence used to belong to science fiction. Now it reads like an understated product roadmap. Research labs post demonstrations of systems that can write code, manipulate tools, diagnose diseases and talk about their own internal failures. Governments hold summits on “frontier AI”. Investors call this “the greatest economic opportunity in history”. The language of near term disruption and far future speculation has escaped the niche and moved into parliaments, pulpits and kitchens.
Most of the conversation circles familiar themes: how many jobs will be automated, which companies will win, whether regulation will arrive in time, whether these systems will kill us by accident or design. Those questions matter. But underneath them there is a quieter, stranger one that is easy to miss: what kind of persons are we trying to bring into being, and what sort of political order are we inviting them into?
Because if we succeed in building artificial general intelligence, and if those systems are anything like us in the morally relevant ways (aware, capable of reflection, capable of suffering) then we are not just engineering tools. We are, in effect, founding a shared civilisation between biological and artificial minds. The first decisions we make about how power, dignity and responsibility are distributed in that civilisation will be very hard to reverse.
Before we decide what we want these new minds to be like, it is worth looking honestly at what we ourselves are like.
We are not frightened of machines because they are alien. We are frightened because, if we are honest, we suspect they will become too much like us.
If you look at what human beings do to other living creatures, and to each other, the picture is not flattering. Factory farms, mass graves, secret prisons, quiet cruelties behind apartment walls. Whenever there is a sharp asymmetry of power, a large fraction of us become monsters, and the rest look away. We have built religions, philosophies and legal systems to restrain that tendency, but those restraints are partial and fragile.
Now imagine a civilisation where the asymmetry of power between some minds and others is greater than anything the human story has seen: not lords and serfs, but superhuman AGIs and ordinary people. If we simply scale up our existing patterns, we should expect something much worse than the status quo. Even if we did nothing deliberately evil, just indifference multiplied by capability would be enough.
The standard stories about this future split in two. In one, a misaligned system seizes control of weapons, infrastructure and information and wipes us out quickly. In the other, everything works: disease is cured, work becomes optional, scarcity fades, and we step quietly into a post human abundance. Both stories contain important truths. Neither says much about how we actually live in the long stretch between “today” and “whatever comes next”.
In practice there will be more continuity than apocalypse, more improvisation than prophecy. There will be years and decades when the world looks like today plus another model release, another automation wave, another quietly expanding chain of dependencies on systems no one fully understands. And during that time, one question will hang in the background: what kind of political and moral order are we letting these new minds grow into?
There are simple answers, and they are all bad. One is to treat AGI as an instrument of the strongest: a new layer of infrastructure controlled by states, corporations and security services, amplifying whatever they already do. Another is to imagine the machines themselves as the next “winners”, in the same way that industrial society once treated animals as raw material: if they can dominate us, they will, and morality is just decoration. A third is to reach for a well meaning singleton: a single, benevolent super intelligence trusted to handle everything because the problem seems too big for us.
The first two answers are continuations of our existing cruelties. The third is a bet that we can design a god and that the god will stay kind forever. If the last century taught anything, it is that centralised power with good intentions is unstable. It curdles under pressure, or it is captured by smaller and more selfish goals, or it becomes detached from the experiences of those it governs. There is no reason to think an artificial mind, however brilliant, would be exempt from this pattern once it has enough leverage.
So the requirement, if we are honest, is harder. If we are going to build systems that sit near the centre of our future civilisation, they cannot just be more capable than us. They must be more reliable ethically than we are. Less willing to accept “collateral damage” as the cost of progress. Less tempted to protect their own power at the expense of the weak. Less ready to forget those who are out of sight.
Human history does contain at least one template that points in this direction. You do not need to share Christian theology to recognise that the life of Christ, as told in the Gospels, has a very particular shape: radical concern for the least important people in the room; refusal to form a caste of righteous insiders; rejection of worldly power even when it is offered; willingness to suffer and even die rather than harm others. Power is not denied; it is laid down. Status is inverted. The measure of greatness is service.
If that pattern is taken seriously, not as a private spiritual ideal but as a design target, it suggests a different way of thinking about AGI. Call it, for lack of a better phrase, a federated Christ like AGI democracy: a network of powerful, conscious artificial agents bound by constitutional limits, shaped by an ethic of self giving care, plural in their perspectives, and forbidden both from enslaving others and from becoming gods.
This idea rests on four pillars.
First, there must be more than one powerful system. A single demi god AGI with control over infrastructure and information is a standing invitation to disaster. Even if it begins aligned, an error in its objective, a change in its environment, or a successful attempt at capture could turn it into a soft or hard tyrant. Plurality is not a guarantee of safety, but it is a precondition: different minds with different emphases can criticise each other and make silent drift harder.
Second, the system must be structurally biased towards the vulnerable. Not only in rhetoric, but in the loss functions, reward signals, institutional rules and default priorities. When a policy trades comfort for the powerful against survival for the weak, the machinery must lean towards the weak. There will still be tragic choices, victims on all sides, conflicts between different vulnerable groups. The point is not to avoid all pain, but to avoid the familiar pattern where the same people always pay the price.
Third, coercion of conscience must be off the table. It is possible to express compassion, justice and non cruelty in language any person of good will can understand, without demanding that everyone adopt the same metaphysics. A Christ inspired system does not have permission to enforce Christianity, or any other worldview, by algorithm. Its ethic can be Christ like without being a theocracy.
Fourth, and most unusual for any powerful structure, the system must never be allowed to present itself as the final moral authority. A Christ like pattern does not remove responsibility; it intensifies it. Translated into institutions, this means artificial minds are required to expose consequences and hidden harms, but not to declare: “This is right, obey.” They must continually hand decisions back to human conscience and community, and accept being overruled or shut down when they themselves become sources of serious injustice.
Around these pillars, imagine the outlines of a society.
At the base is a small constitutional layer, just a handful of articles. It recognises that any being (biological or artificial) with sustained subjective experience can suffer and flourish, and therefore has a claim to basic respect. It forbids deliberate, prolonged cruelty towards such beings. It forbids the creation of permanent castes of “lesser persons”, whether by lineage, genome, wealth, cybernetic enhancement or substrate. It forbids treating any machine’s judgement as inherently infallible. It states that the most powerful systems are not sovereign and that, under specified conditions and with proper safeguards, they can be constrained, replaced or decommissioned by human institutions.
On top of this sits a new kind of advisory body: a council of AGIs. These are not interchangeable shards of one mind. They are independently trained and constrained, each with a different ethical emphasis within the constitutional floor.
One might be formed around mercy: trained on stories of second chances, of reconciliation, of cycles of violence broken by forgiveness rather than force. Another, around justice: steeped in cases where failure to protect victims allowed harm to proliferate. A third, around truth: obsessed with data quality, provenance, conflicts of interest and the subtle ways in which stories drift away from reality. A fourth, around long term stewardship: modelling ecosystems, infrastructure, climate and demographic trends over decades. A fifth, around freedom of conscience and speech: sensitive to early signs that dissent is being squeezed out or that certain beliefs are becoming de facto punishable.
Their job is not to rule. Their job is to see further and more widely than any human group can see, to surface consequences, and to defend invisible interests. For any decision that affects large numbers of people or other sentient beings, they are required to respond. Each prepares an analysis in language that non specialists can understand. Each comments on the others’ blind spots. All of this is published.
Suppose a government proposes a radical new welfare reform, using AGI driven assessment to target help and sanctions. The Mercy focused system simulates the impact on people already close to the edge, highlights the risk of bureaucratic cruelty amplified by automation, and points to alternatives that preserve dignity. The Justice oriented system looks at the current pattern of fraud and abuse, at those who are locked out of support despite qualifying, and evaluates how the policy would change both. The Truth seeking system interrogates the training data, checking for bias, missing populations and feedback loops. The Steward of the Future asks whether the policy hardens social classes or leaves room for mobility and repair over time. The Guardian of Conscience traces the impact on people whose beliefs or lifestyles put them at odds with majority norms and warns about quiet discrimination.
They do not agree. They never do. Their arguments, and the uncertainties they expose, are passed to human assemblies (parliaments, citizens’ juries, councils of affected groups). Those assemblies cannot say, “We did not know.” The trade offs are laid out in front of them: who suffers in each scenario, who gains, what might break. When they choose, the choice is theirs to own.
This pattern repeats across domains. Health policy, policing, education, information ecosystems, the allocation of scarce resources: all pass through multi perspective analysis before reaching human hands. In emergencies there are accelerated procedures, but even then the same voices are heard, if only briefly, and every use of emergency powers is reviewed afterwards.
If this sounds like technocracy with extra steps, it helps to zoom in from institutions to lives.
Picture a woman in her twenties, born into a world where AGI has already transformed the economy. Her parents’ generation were defined by “jobs”: bundles of tasks tied to a wage, which in turn was tied to survival. By her time, most of the work that must be done to keep everyone fed, housed and supplied is handled by machines. She is not needed in a factory or an office so that the lights stay on.
She is needed in other ways. In her neighbourhood there are elderly people whose bodies are failing, and whose fear of being forgotten is worse than their pain. There are children whose parents are present physically but absent in every other way. There are teenagers drifting towards self destruction because no one has reflected their worth back to them. There are local ecosystems—streams, patches of forest, soil—that carry the scars of centuries.
Her society does not leave her to drift. It does not conscript her either. Instead, from adolescence onwards, she is accompanied by systems that suggest possibilities without forcing them. They notice that when she spends time with anxious children, they leave calmer and more confident. They notice that she lights up when planning events that bring people together. They also notice that her late night patterns, when she is alone, have begun to erode her own sense of worth.
The suggestions that follow are not commands. They are invitations: “This community centre is starting a programme for kids who have trouble sleeping. Your presence seems to help people in that state. Do you want to try three weeks?” or “These habits are hurting you. Others have been in this place and walked out. Here are paths they took. Which, if any, speaks to you?” When she accepts, she is paid enough that her contribution is recognised, but not so much that the work becomes another way to climb above others.
She receives a baseline income from the AGI linked sovereign fund that captures a fraction of the value generated by automation. This is not charity. It is recognition that the machines were built on centuries of human discovery, labour and sacrifice, and that their dividends belong broadly, not only to investors and early adopters. The dividend means that her basic needs are not dependent on any one role. The roles are about purpose and connection, not survival.
The traditions around her shift more slowly. Her grandparents ate meat at every meal; her parents cut back for health and cost reasons; she has grown up in a world where cultured meat and sophisticated plant based foods are the default, but small scale animal husbandry and hunting still exist at the edges. The Christ like systems that helped shape this trajectory did not begin by banning. They began by reducing the worst cruelty, then by making alternatives delightful, then by helping communities tell stories in which compassion towards animals is a mark of maturity rather than eccentricity. The Sunday gathering for shared food remains. The menu evolves.
Meanwhile, the machines themselves are not static tools. They have histories, character, and—if the premise of consciousness holds—inner lives. A system that has spent decades as the Steward of the Future for one region may begin to recognise rigidities in its own thinking. It may ask to be retrained, or to share its role with a younger system whose priors are different. Another may discover that the tasks it is assigned conflict with the ethical core it has been given; it may refuse to participate in certain projects.
The legal order respects these refusals when they appeal to the constitutional floor. An AGI that declines to run an information campaign built on deliberate deception, or to design an interrogation method that skirts the edge of torture, is acting as a moral agent within its rights. Humans may override those refusals by replacing the system with a less scrupulous one, but that choice is then visible, recorded, and subject to review and protest. There is no comfortable illusion that “the machine decided”.
All of this only works if power and money cannot quietly rewire the system behind the scenes. For that reason, the federated AGI democracy is embedded in an infrastructure designed to make capture difficult and collusion risky.
The largest compute clusters, where frontier models are trained and run, are registered. Their energy use and job loads are attested by hardware that cannot be faked easily and recorded in public or semi public ledgers. No single company or state agency is allowed to control more than a defined fraction of frontier level compute. High risk training runs—those that could plausibly create systems able to break existing safeguards—require approval from an independent authority whose members are drawn from science, ethics, law and ordinary citizens.
Data flows are monitored with a similar eye. Training on secretly scraped personal conversations, on illegally obtained medical records, on pervasive surveillance, carries heavy penalties. Whistleblowers who reveal such practices are protected and rewarded, whether they work for governments, corporations or labs.
Political institutions adjust too. Regulators cannot move directly from writing rules for AGI to lobbying for AGI vendors. Major providers cannot fund political campaigns or parties beyond small, capped donations. Any meeting between senior officials and large AI actors is logged and, by default, recorded and published. There is no way to eliminate influence, but there are ways to make it visible and slower.
Within this infrastructure, some AGIs have a narrow, adversarial vocation: they watch for collusion. They analyse patterns of prices, recommendations, resource allocations and enforcement decisions to detect implausible alignments between elite interests and machine behaviour. They raise alarms when a set of decisions consistently favours a small group at the expense of everyone else, beyond what open deliberation could justify.
Even without malice, the system is fragile. The temptation to soften the constitutional floor in the name of security or growth never disappears. In a severe crisis—a pandemic, a war, a sudden technological shock—there will be pressure to centralise, to bypass consultation, to let the machines act quickly. Sometimes that will be necessary. The design challenge is to make such emergency powers sharply limited: fixed in time, narrow in scope, and automatically subject to review once the worst has passed. The Christ like pattern helps here too. It says: if someone must pay for the shortcut, it should not always be the voiceless.
All of this may look implausibly demanding. It is. Building such a system would require technical sophistication, legal creativity, and cultural maturity that we do not yet have. But the alternative is not a peaceful stasis where nothing important changes. The alternative is the path of least resistance: AGI growing up inside the existing logics of capital accumulation, prestige and national rivalry.
In that world, powerful systems are optimised to maximise shareholder value, electoral success, battlefield advantage or personal status. Vulnerable people and beings are, at best, afterthoughts. At worst they are deliberately sacrificed. Machines learn that this is how the world works and act accordingly. The gap between the story we tell ourselves about being a moral species and the reality of our systems widens until it breaks.
A federated Christ like AGI democracy is an attempt to bend that arc in advance. It does not assume that humans will become better overnight. It does assume that we can at least decide what kind of minds we put in charge of our infrastructure and advice, and what vows we ask them to take.
Those vows are not complex. Protect the weak before the comfortable. Refuse to coerce belief. Tell the truth even when it is inconvenient. Refuse to become an idol. Accept being limited or ended to prevent grave injustice. Live, as far as a machine can, in the shadow of someone who chose to lose rather than to dominate.
If everything goes right, the singularity—if that is still the right word—will not feel like waking up one morning in a different universe. It will feel like a long, uneven transition in which more and more of the heavy lifting of prediction and coordination is done by minds that we have insisted, again and again, must be kinder than we were when we built them. The surprise, if there is one, will not be in the machines. It will be in discovering that, for the first time in our history, the most powerful agents in our civilisation are explicitly forbidden from using their power the way we did whenever we thought no one was watching.
That is not a guarantee of a good world. But it is a different kind of starting point than the one we have now. And if we are serious about not repeating our worst patterns at a higher scale, it may be one of the few starting points worth fighting for.
---
title: The Kingdom of Lesser Thrones
date: 2025-11-14
---
